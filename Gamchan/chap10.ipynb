{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+QNtVcezfuVb6NBkrkRuz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gsgh3016/Pytorch-study/blob/main/Gamchan/chap10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10-01 언어 모델이란?\n",
        "- 통계를 이용한 방법\n",
        "- 인공 신경망을 이용한 방법\n",
        "\n",
        "## 1. 언어 모델\n",
        "- 언어 모델: 단어 시퀀스에 확률을 할당하는 모델\n",
        "- 유형 1) 이전 단어들이 주어졌을 때 **다음 단어 예측**\n",
        "- 유형 2) 양쪽 단어로 **가운데 비어있는 단어 예측** &rarr; BERT\n",
        "\n",
        "## 2. 단어 시퀀스의 확률 할당\n",
        "\n",
        "### a. 기계 번역\n",
        "> P(\"나는 버스를 탔다\") > P(\"나는 버스를 태운다\")`\n",
        "\n",
        "### b. 오타 교정\n",
        "> 선생님이 교실로 부리나케  P(달려갔다) > P(잘려갔다)\n",
        "\n",
        "### c. 음성 인식\n",
        "> P(나는 메롱을 먹는다) < P(나는 메론을 먹는다)\n",
        "\n",
        "## 3. 주어진 이전 단어들로부터 다음 단어 예측하기\n",
        "\n",
        "> $P(\\text{\"나는 밥을 먹는다\"}) = P(\\text{\"나는\"}) \\times P(\\text{\"밥을\"} | \\text{\"나는\"}) \\times P(\\text{\"먹는다\"} | \\text{\"나는 밥을\"})\n",
        "$\n",
        "\n",
        "## 4. 언어 모델의 간단한 직관\n",
        "\n",
        "> 비행기를 타려고 공항에 갔는데 지각을 하는 바람에 비행기를 [?]\n",
        "\n",
        "위 문장에서 [?]에 들어갈 단어는?\n",
        "\n",
        "사람의 경우 쉽게 \"놓쳤다\"라고 예상할 수 있음.<br>\n",
        "&rarr; **지식에 기반**하여 나올 수 있는 여러 단어를 후보<br>\n",
        "&rarr; **가장 확률이 높은 단어** 선택\n"
      ],
      "metadata": {
        "id": "SHo4ScC5ridP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10-02 통계적 언어 모델(Statistical Language Model, SLM)\n",
        "\n",
        "## 1. 조건부 확률\n",
        "> $p(B|A) = P(A,B)/P(A)$\n",
        "> $P(A,B) = P(A)P(B|A)$\n",
        "\n",
        "**조건부 확률의 연쇄 법칙(chain rule)**\n",
        "\n",
        "> $P(x_1, x_2, x_3 ... x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1 ... x_{n-1})$\n",
        "\n",
        "## 2. 문장에 대한 확률\n",
        "\n",
        "> $P(\\text{An})  ×  P(\\text{adorable|An})  ×  P(\\text{little|An adorable}) × P(\\text{boy|An adorable little}) ×  P(\\text{is|An adorable little boy})$\n",
        "\n",
        "## 3. 카운트 기반의\n",
        "\n",
        "> \\begin{align}\n",
        "> P\\text{(is|An adorable little boy}) = \\frac{\\text{count(An adorable little boy is})}{\\text{count(An adorable little boy })}\n",
        "> \\end{align}\n",
        "\n",
        "## 4. 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)\n",
        "\n",
        "- 카운트 기반 접근 방식 &rarr; 데이터가 충분히 커야 함\n",
        "- 코퍼스(corpus)에 해당 데이터가 없지만, 문법/문맥 상 맞는 말 &rarr; 확률 0\n",
        "- 인공 신경망 언어 모델"
      ],
      "metadata": {
        "id": "CTLkkh77utx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10-03 N-gram 언어 모델(N-gram Language Model)\n",
        "\n",
        "- 이전 단어 모두 고려 X &rarr; 일부 단어만 고려 &rarr; 몇 개: $n$\n",
        "\n",
        "## 1. 코퍼스에서 카운트하지 못하는 경우의 감소.\n",
        "\n",
        "> $P(\\text{is|An adorable little boy}) \\approx\\ P(\\text{is|boy})$\n",
        "\n",
        "&rarr; 지나친 일반화?\n",
        "\n",
        "> $P(\\text{is|An adorable little boy}) \\approx\\ P(\\text{is|little boy})$\n",
        "\n",
        "&rarr; 참조 단어 조절\n",
        "\n",
        "## 2. N-gram Language Model의 한계\n",
        "\n",
        "### (1) 희소 문제(Sparsity Problem)\n",
        "\n",
        "### (2) n을 선택하는 것은 trade-off 문제\n",
        "- n을 작게 &rarr; 훈련 코퍼스 카운트 👍 근사 정확도 👎\n",
        "- n은 최대 5를 넘게 잡아서는 안 된다고 권장\n",
        "- 펄플렉서티(perplexity)는 수치가 낮을수록 더 좋은 성능\n",
        "\n",
        "## 4. 적용 분야(Domain)에 맞는 코퍼스의 수집\n",
        "\n",
        "## 5. 인공 신경망을 이용한 언어 모델(Neural Network Based Language Model)"
      ],
      "metadata": {
        "id": "z1ojMDjPw1WO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10-04 한국어에서의 언어 모델(Language Model for Korean Sentences)\n",
        "\n",
        "## 1. 한국어는 어순이 중요하지 않다.\n",
        "\n",
        "## 2. 한국어는 교착어이다.\n",
        "\n",
        "- 교착어: 접사를 붙여 그 의미나 문법적 기능을 확장하는 언어\n",
        "- 한국어 토큰화: 접사, 조사 등 분리 작업 중요!\n",
        "\n",
        "## 3. 한국어는 띄어쓰기가 제대로 지켜지지 않는다.\n"
      ],
      "metadata": {
        "id": "gNutxlWszatW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10-05 펄플렉서티(Perplexity, PPL)\n",
        "\n",
        "## 1. 언어 모델의 평가 방법(Evaluation metric) : PPL\n",
        "\n",
        "-  'perplexed': '헷갈리는'\n",
        "\n",
        "> \\begin{align}\n",
        "PPL(W)=P(w_{1}, w_{2}, w_{3}, ... , w_{N})^{-\\frac{1}{N}}=\\sqrt[N]{\\frac{1}{P(w_{1}, w_{2}, w_{3}, ... , w_{N})}}\n",
        "\\end{align}\n",
        "\n",
        "> \\begin{align}\n",
        "\\sqrt[N]{\\frac{1}{P(w_{1}, w_{2}, w_{3}, ... , w_{N})}}= \\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}P(w_{i}| w_{1}, w_{2}, ... , w_{i-1})}}\n",
        "\\end{align}\n",
        "\n",
        "&#42; PPL 수식 해석: 특정 코퍼스에서 단어들 간 연결 확률의 역수 &rarr; Cross Entropy 함수와 수식 유사\n",
        "\n",
        "> \\begin{align}CrossEntropy(T,Q)=−∑_x P(x)log_2Q(x)\\end{align}\n",
        "\n",
        "> \\begin{align}\n",
        "PPL(W)=2^{CrossEntropy(T,Q)}\n",
        "\\end{align}\n",
        "\n",
        "&#42; 정성적 의미의 PPL: 모델이 다음 단어를 예측할 때 고려하는 평균적인 단어의 수 &rarr; 예측을 하는데 고려하는 단어의 수가 많다? &rarr; 모델의 **혼란**을 나타내는 지표 (chatGPT)"
      ],
      "metadata": {
        "id": "dYzWYcIbz6rM"
      }
    }
  ]
}